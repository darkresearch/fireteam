---
title: "Performance Benchmarks"
description: "Detailed metrics, cycle analysis, and efficiency benchmarks from Fireteam testing"
---

## Overview

This page provides detailed performance metrics from Fireteam's comprehensive testing across 11 projects. Use these benchmarks to set expectations and optimize your configurations.

## Core Metrics

### Cycle Performance

<CardGroup cols={3}>

<Card title="Average Cycles" icon="rotate">
  **3.7 cycles** per project
</Card>

<Card title="Modal Cycles" icon="chart-bar">
  **3 cycles** (most common)
</Card>

<Card title="Best Case" icon="bolt">
  **1 cycle** (27% of projects)
</Card>

</CardGroup>

### Time Performance

<CardGroup cols={3}>

<Card title="Average Time" icon="clock">
  **~50 minutes** per project
</Card>

<Card title="Fastest" icon="gauge-simple-high">
  **20 minutes** (Web Scraper)
</Card>

<Card title="Per Cycle" icon="stopwatch">
  **~13-15 minutes** average
</Card>

</CardGroup>

### Quality Metrics

<CardGroup cols={3}>

<Card title="Completion Rate" icon="percent">
  **94.1%** average
</Card>

<Card title="Success Rate" icon="check">
  **100%** (≥90% threshold)
</Card>

<Card title="Consistency" icon="chart-line">
  **±2.9%** std deviation
</Card>

</CardGroup>

## Time Analysis by Cycle Count

### 1-Cycle Projects (27%)

**Projects:** REST API Server, Task Manager CLI, Web Scraper

| Phase | Duration | Percentage |
|-------|----------|------------|
| Planning | 3-5 min | 20% |
| Execution | 15-20 min | 70% |
| Review | 2-4 min | 10% |
| **Total** | **20-30 min** | **100%** |

**Characteristics:**
- Clear, focused requirements
- Standard tech stack (Python + common libraries)
- 200-500 lines of code
- Minimal dependencies

### 2-Cycle Projects (18%)

**Projects:** Weather CLI, Calculator

| Phase | Cycle 0 | Cycle 1 | Total |
|-------|---------|---------|-------|
| Planning | 4 min | 3 min | 7 min |
| Execution | 16 min | 10 min | 26 min |
| Review | 3 min | 3 min | 6 min |
| **Total** | **23 min** | **16 min** | **~40 min** |

**Characteristics:**
- Slight refinement needed after initial implementation
- Additional error handling or tests
- 85-92% in cycle 0, 95%+ in cycle 1

### 3-Cycle Projects (45%)

**Projects:** Solana Price Checker, CSV Analyzers, JSON Parser, Hello World

| Phase | Cycle 0 | Cycle 1 | Cycle 2 | Total |
|-------|---------|---------|---------|-------|
| Planning | 5 min | 4 min | 3 min | 12 min |
| Execution | 18 min | 12 min | 8 min | 38 min |
| Review | 4 min | 3 min | 3 min | 10 min |
| **Total** | **27 min** | **19 min** | **14 min** | **~60 min** |

**Characteristics:**
- More complex logic or edge cases
- API integration with error handling
- Data processing with validation
- Incremental refinement: 85% → 90% → 95%

### Multi-Cycle Projects (Outliers)

**Project:** GitHub Analyzer (19 cycles, ~5 hours)

**Time breakdown:**
- Cycles 0-7: Environment setup failures (~2 hours wasted)
- Cycle 8: Node.js binary installation breakthrough (~15 min)
- Cycles 9-19: Productive development (~3 hours)

<Warning>
Without sudo access or pre-installed Node.js, TypeScript projects can waste 5-10 cycles on environment setup. See [Sudo Setup](/configuration/sudo-setup).
</Warning>

## Cycle Duration Patterns

### Phase Duration by Agent

| Agent | Minimum | Typical | Maximum | Timeout |
|-------|---------|---------|---------|---------|
| **Planner** | 2 min | 4-6 min | 10 min | 10 min |
| **Executor** | 5 min | 15-20 min | 30 min | 30 min |
| **Reviewer** | 2 min | 3-5 min | 10 min | 10 min |

### Cycle Duration Trends

**Early cycles (0-1):**
- Longer planning (5-7 min) - understanding codebase
- Longer execution (18-25 min) - building structure
- Longer review (4-6 min) - comprehensive check

**Later cycles (2+):**
- Shorter planning (2-4 min) - focused refinement
- Shorter execution (8-15 min) - targeted fixes
- Consistent review (3-5 min) - validation

**Pattern:**
```
Cycle 0: 27 min (setup + core implementation)
Cycle 1: 19 min (refinement)
Cycle 2: 14 min (polish)
Cycle 3+: 12-15 min (validation)
```

## Efficiency Metrics

### Code Production Rate

**Lines of code per minute:**
- Simple projects: ~10-15 LOC/min
- Medium complexity: ~6-10 LOC/min
- Complex projects: ~3-6 LOC/min

**Example - REST API Server (1 cycle, 25 min):**
- Total code: ~400 lines (including tests)
- Rate: ~16 LOC/min
- Quality: 92% completion, all tests passing

### Completion Velocity

**Completion % per cycle:**

| Project Type | Cycle 0 | Cycle 1 | Cycle 2 | Cycle 3 |
|--------------|---------|---------|---------|---------|
| Simple CLI | 92-100% | Validation | - | - |
| API/DB | 88-92% | 93-95% | 96%+ | - |
| Complex/TS | 75-85% | 85-90% | 90-93% | 94-96% |

**Velocity patterns:**
- **Fast start:** Most projects reach 85-92% in cycle 0
- **Refinement:** +3-5% per cycle
- **Diminishing returns:** Harder to gain % after 90%

### Resource Efficiency

**CPU utilization:**
- Planning: 20-40% (analysis, Claude API calls)
- Execution: 40-80% (building, testing, installing)
- Review: 30-50% (testing, analysis)

**Memory usage:**
- Baseline: ~500MB (orchestrator + state)
- Peak: ~2-4GB (including agent processes)
- TypeScript builds: Up to 6GB (npm install, tsc)

**Network usage:**
- Claude API calls: ~10-30 per cycle
- Bandwidth: ~5-15MB per cycle (API responses)
- Package downloads: Variable (0-500MB for dependencies)

## Comparison: Fireteam vs. Manual Development

### Time Comparison

**Simple CLI (e.g., Weather App):**
- **Manual:** 2-4 hours (experienced dev)
- **Fireteam:** 40 minutes (2 cycles)
- **Speedup:** ~3-6x faster

**REST API (e.g., Todo API):**
- **Manual:** 4-8 hours (with tests, docs)
- **Fireteam:** 25 minutes (1 cycle)
- **Speedup:** ~10-20x faster

**Complex App (e.g., GitHub Analyzer):**
- **Manual:** 2-3 days (8-24 hours)
- **Fireteam:** 5 hours (19 cycles, including debugging)
- **Speedup:** ~2-5x faster (even with blockers)

<Info>
Fireteam excels at "first draft" implementation speed. Manual development may still be needed for highly specialized features or optimization.
</Info>

### Quality Comparison

**Code quality indicators:**

| Metric | Manual (Typical) | Fireteam | Notes |
|--------|------------------|----------|-------|
| Test coverage | 60-70% | 70-85% | Fireteam includes tests by default |
| Error handling | Varies | Consistent | Fireteam adds try-catch, validation |
| Documentation | Often minimal | Complete | README, docstrings generated |
| Code style | Personal | Consistent | Follows language conventions |

## Optimization Insights

### When to Expect Fast Completion (1-2 cycles)

✅ Clear, specific goals
✅ Standard tech stack (Python + pip packages)
✅ No system dependencies
✅ Focused scope (single primary function)
✅ 200-500 lines of code

**Example goal:**
```
Build a Python CLI that checks Bitcoin prices using CoinGecko API.
Include error handling and colored output.
```

### When to Expect Longer Development (3-5 cycles)

⚠️ Vague or complex requirements
⚠️ Multiple integrated systems
⚠️ Novel tech combinations
⚠️ 1000+ lines of code
⚠️ Advanced algorithms

**Example goal:**
```
Build a full-stack application with multiple features,
authentication, database, and API integration.
```

### When to Expect Issues (5+ cycles)

❌ TypeScript without Node.js pre-installed
❌ System packages requiring sudo (without passwordless sudo)
❌ Extremely vague goals ("make me an app")
❌ Conflicting requirements
❌ Unsupported tech stacks

## Scaling Expectations

### By Project Size

| Size | LOC | Cycles | Time | Examples |
|------|-----|--------|------|----------|
| **Tiny** | &lt;200 | 1-2 | 20-40 min | Hello World, Simple CLI |
| **Small** | 200-500 | 1-3 | 30-60 min | Weather CLI, Calculator |
| **Medium** | 500-1000 | 2-4 | 60-120 min | REST API, Task Manager |
| **Large** | 1000-2000 | 3-6 | 2-4 hours | GitHub Analyzer, Full APIs |
| **Very Large** | 2000+ | 6-10+ | 4-8+ hours | Complex full-stack apps |

### By Complexity

**Simple (1-3 cycles):**
- Single file or simple module structure
- Standard libraries only
- Basic CRUD or data processing
- Clear input/output

**Medium (3-5 cycles):**
- Multiple modules
- External API integration
- Database operations
- Error handling + tests

**Complex (5-10 cycles):**
- Multi-language or framework integration
- Advanced algorithms
- Performance optimization
- Comprehensive test suites

**Very Complex (10+ cycles):**
- Full-stack applications
- Microservices architecture
- Complex state management
- Production deployment setup

## Timeout Impact Analysis

### Planner Timeout: 10 minutes

**Utilization:**
- Average: 4-6 minutes (40-60%)
- 95th percentile: 8 minutes (80%)
- Max observed: 9 minutes

**Timeout incidents:** 0/41 cycles (0%)

**Conclusion:** 10 minutes is adequate

### Executor Timeout: 30 minutes

**Utilization:**
- Average: 15-20 minutes (50-67%)
- 95th percentile: 25 minutes (83%)
- Max observed: 28 minutes (GitHub Analyzer)

**Timeout incidents:** 0/41 cycles (with 30min)
**Previous incidents (10min):** ~5 cycles timed out (GitHub Analyzer)

**Conclusion:** 30 minutes necessary for reliability

### Reviewer Timeout: 10 minutes

**Utilization:**
- Average: 3-5 minutes (30-50%)
- 95th percentile: 7 minutes (70%)
- Max observed: 8 minutes

**Timeout incidents:** 0/41 cycles (0%)

**Conclusion:** 10 minutes is adequate

## Cost Analysis (Claude API)

### API Call Estimates

**Per cycle:**
- Planning: 3-5 API calls
- Execution: 5-15 API calls
- Review: 2-4 API calls
- **Total:** ~10-25 calls per cycle

**Per project (3.7 cycles avg):**
- **Total API calls:** ~40-90 calls
- **Token usage:** Varies by project (50k-200k tokens)

### Cost Estimates

**Assuming Claude API pricing:**
- Input: $3/million tokens
- Output: $15/million tokens

**Average project:**
- Input tokens: ~100k
- Output tokens: ~50k
- **Cost:** ~$0.30-1.50 per project

**Cost per hour of development time:**
- 50-minute project @ $0.50 = $0.60/hour
- Compare to developer at $50-150/hour

<Tip>
Fireteam delivers ROI of 50-100x in development cost savings, even accounting for API usage.
</Tip>

## Best Practices for Optimal Performance

### 1. Write Specific Goals

**Vague:**
```
Build a web app
```
**Specific:**
```
Build a Python Flask app for managing books with CRUD operations and SQLite
```

**Impact:** Specific goals reduce cycles by 30-50%

### 2. Choose Optimal Tech Stack

**Fast (1-3 cycles):**
- Python + stdlib or pip packages
- Established frameworks (FastAPI, Flask)
- SQLite for databases

**Moderate (3-5 cycles):**
- TypeScript with pre-installed Node.js
- Less common libraries
- PostgreSQL/MySQL

**Slow (5+ cycles):**
- TypeScript without Node.js
- Novel combinations
- Bleeding-edge frameworks

### 3. Configure Environment

**Before starting:**
- Install language runtimes (Node.js for TS)
- Configure sudo access
- Ensure adequate timeouts

**Impact:** Saves 5-10 cycles on environment issues

### 4. Monitor Early Cycles

**Check after cycle 1:**
- Is completion >85%?
- Are there environment errors?
- Is scope appropriate?

**Impact:** Early intervention prevents wasted cycles

## Benchmarking Your Projects

### Collect Your Metrics

After each project, record:

```bash
# Extract metrics from logs
grep "CYCLE" orchestrator.log | wc -l  # Cycle count
grep "Completion:" orchestrator.log    # Completion %
# Calculate total time from timestamps
```

### Compare to Benchmarks

| Your Metric | Benchmark | Status |
|-------------|-----------|--------|
| Cycles | 3.7 avg | ✅ At or below = Good |
| Completion | 94% avg | ✅ At or above = Good |
| Time | ~50 min | ✅ Similar = Expected |

### Identify Issues

**More cycles than expected?**
- Check for environment issues
- Review goal clarity
- Verify timeout configuration

**Lower completion?**
- Check for scope creep
- Review agent drift indicators
- Consider tech stack complexity

## Next Steps

<CardGroup cols={2}>

<Card title="Test Results" icon="chart-bar" href="/performance/test-results">
  Detailed test results and project examples
</Card>

<Card title="Configuration" icon="sliders" href="/configuration/config-file">
  Optimize timeouts and thresholds
</Card>

<Card title="Timeouts" icon="clock" href="/configuration/timeouts">
  Fine-tune agent timeout values
</Card>

<Card title="Troubleshooting" icon="wrench" href="/troubleshooting/troubleshooting">
  Resolve performance issues
</Card>

</CardGroup>
