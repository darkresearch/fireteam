{
    "results": {
        "tool": {
            "name": "pytest",
            "version": "8.4.1"
        },
        "summary": {
            "tests": 6,
            "passed": 5,
            "failed": 1,
            "skipped": 0,
            "pending": 0,
            "other": 0,
            "start": 1768018961.6908317,
            "stop": 1768018961.8202503
        },
        "tests": [
            {
                "name": "test_outputs.py::test_output_files_exist",
                "status": "passed",
                "duration": 0.0007616249999955471,
                "start": 1768018961.7685826,
                "stop": 1768018961.7698135,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_input_data_integrity",
                "status": "passed",
                "duration": 0.0013987500001348963,
                "start": 1768018961.7700183,
                "stop": 1768018961.7716012,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_generate_and_schema",
                "status": "passed",
                "duration": 0.008602831999951377,
                "start": 1768018961.7718263,
                "stop": 1768018961.7806554,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_solution_shape_feasibility_and_batch_consistency",
                "status": "passed",
                "duration": 0.0078353750000133,
                "start": 1768018961.7809448,
                "stop": 1768018961.7889624,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_solution_coverage_no_duplicates",
                "status": "passed",
                "duration": 0.006527875999609023,
                "start": 1768018961.7891657,
                "stop": 1768018961.7959769,
                "retries": 0,
                "file_path": "test_outputs.py"
            },
            {
                "name": "test_outputs.py::test_performance_thresholds",
                "status": "failed",
                "raw_status": "call_failed",
                "duration": 0.0128305419998469,
                "start": 1768018961.7962337,
                "stop": 1768018961.8197565,
                "retries": 0,
                "file_path": "test_outputs.py",
                "trace": "def test_performance_thresholds() -> None:\n        \"\"\"Test solution meets performance thresholds described in task.yaml.\n    \n        **Performance Thresholds:**\n        - Bucket 1: cost \u2264 3.0e11, pad_ratio \u2264 0.055, p95_latency \u2264 2.1e6 ms,\n          sequential_timecost \u2264 2.7e8 ms\n        - Bucket 2: cost \u2264 4.8e10, pad_ratio \u2264 0.15, p95_latency \u2264 2.1e5 ms,\n          sequential_timecost \u2264 3.2e7 ms\n    \n        **Optimization Verification:**\n        - Global shape constraint: \u2264 8 unique shapes across all buckets\n    \n        \"\"\"\n    \n        req1_path = \"/app/task_file/input_data/requests_bucket_1.jsonl\"\n        plan1_path = \"/app/task_file/output_data/plan_b1.jsonl\"\n        m_b1 = _plan_metrics(req1_path, plan1_path)\n    \n        req2_path = \"/app/task_file/input_data/requests_bucket_2.jsonl\"\n        plan2_path = \"/app/task_file/output_data/plan_b2.jsonl\"\n        m_b2 = _plan_metrics(req2_path, plan2_path)\n    \n        # Validate against tight absolute performance thresholds\n        # Bucket 1\n>       assert m_b1[\"cost\"] <= 3.0e11, f\"bucket1 cost {m_b1['cost']} > 3.0e11\"\nE       AssertionError: bucket1 cost 305900586466.0 > 3.0e11\nE       assert 305900586466.0 <= 300000000000.0\n\n/tests/test_outputs.py:322: AssertionError",
                "message": "The test failed in the call phase due to an assertion error"
            }
        ]
    }
}